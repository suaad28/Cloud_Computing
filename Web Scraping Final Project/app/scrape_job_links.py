import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse


def scrape_job_links(list_of_all_URLs):
    """
    function to scrape individual job postings
    (via another function 'scrape_job_info')

    gets HTML of each search page from the list
    generated by the function 'get_all_search_pages',
    parses links to individual job postings
    from search results pages, and feeds them
    to function 'scrape_job_info'

    Input arguments: 'List_of_all_URLs'   -- list  -- list containing URLs of all pages with job search results
    """
    # loop over all pages in 'List_of_all_URLs' to extract links to each job posting
    job_url_list = []
    for page_url in list_of_all_URLs:

        # get the HTML of the search results page
        page = requests.get(page_url)
        content = page.text
        # make a soup out of the HTML
        soup = BeautifulSoup(content, 'lxml')

        # find all <div> tags containing each job posting links and feed them to the function 'scrape_job_info'
        job_search_results = soup.find_all(lambda tag: tag.name == 'div' and tag.get('class') == ['title'])

        for job in job_search_results:

            # extract the individual job posting link from a <div> tag
            job_url = urlparse("https://www.indeed.ca" + job.find('a')['href'])

            if len(job_url.geturl()) < 300:

                job_url_list.append(job_url.geturl())

    return job_url_list
